registryCredentials:
  registry: "${ZARF_REGISTRY}"
  username: "zarf-pull"
  password: "${ZARF_REGISTRY_AUTH_PULL}"

# Openshift Container Platform Feature Toggle
openshift: false

git:
  existingSecret: "private-git-server"

networkPolicies:
  enabled: true
  # When in prod use a real CIDR. Don't do this, it isn't secure.
  controlPlaneCidr: "0.0.0.0/0"
  nodeCidr: "0.0.0.0/0"
  vpcCidr: "0.0.0.0/0"

istio:
  enabled: true
  ingressGateways:
    public-ingressgateway:
      type: "LoadBalancer"
      kubernetesResourceSpec:
        # Uncomment this section if you are on EKS and need to use an Internal Load Balancer instead of the default Classic Load Balancer
#        serviceAnnotations:
#          # Use an Internal Load Balancer rather than a public one
#          service.beta.kubernetes.io/aws-load-balancer-internal: 'true'
#          # Enable cross zone load balancing
#          service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: 'true'
        resources:
          requests:
            cpu: "100m"
            memory: "512Mi"
          limits:
            cpu: "500m"
            memory: "512Mi"
  values:
    istiod:
      resources:
        requests:
          cpu: "100m"
          memory: "1Gi"
        limits:
          cpu: "500m"
          memory: "1Gi"
      hpaSpec:
        maxReplicas: 1
    values:
      global:
        proxy:
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 100m
              memory: 256Mi

  gateways:
    public:
      ingressGateway: "public-ingressgateway"
      hosts:
        - gitlab.{{ .Values.domain }}
        - jenkins.{{ .Values.domain }}
        - jira.{{ .Values.domain }}
        - confluence.{{ .Values.domain }}
        - registry.{{ .Values.domain }}
        - artifactory.{{ .Values.domain }}

istiooperator:
  enabled: true
  values:
    operator:
      resources:
        limits:
          cpu: "500m"
          memory: "256Mi"
        requests:
          cpu: "100m"
          memory: "256Mi"

jaeger:
  enabled: true
  values:
    istio:
      mtls:
        mode: STRICT
    jaeger:
      spec:
        allInOne:
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "128Mi"
        collector:
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "128Mi"
    resources:
      limits:
        cpu: "500m"
        memory: "128Mi"
      requests:
        cpu: "100m"
        memory: "128Mi"

kiali:
  enabled: true
  values:
    istio:
      mtls:
        mode: PERMISSIVE
    resources:
      requests:
        cpu: "100m"
        memory: "512Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"
    cr:
      spec:
        deployment:
          resources:
            requests:
              cpu: "200m"
              memory: "512Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"

clusterAuditor:
  enabled: false

gatekeeper:
  enabled: false

logging:
  enabled: false

eckoperator:
  enabled: false

fluentbit:
  enabled: false

kyverno:
  enabled: true
  values:
    replicaCount: 5
    resources:
      limits:
        cpu: "2"
        memory: 512Mi
      requests:
        cpu: "1"
        memory: 512Mi

kyvernopolicies:
  enabled: true
  values:
    policies:
      disallow-nodeport-services:
        exclude:
          any:
          - resources:
              namespaces:
              - "zarf"
      disallow-privilege-escalation:
        exclude:
          any:
          - resources:
              namespaces:
              - artifactory
              - confluence
              - gitlab
              - jira
              kinds:
              - Pod
              selector:
                matchLabels:
                  application: spilo
              names:
              - "acid*"
      restrict-host-path-mount:
        exclude:
          any:
          - resources:
              namespaces:
              - "kube-system"
              - "local-path-storage"
              - "zarf"
        parameters:
          allow:
          - "/var/lib/rancher/k3s/storage/*"
          - "/var/local-path-provisioner/*"
      restrict-host-path-write:
        exclude:
          any:
          - resources:
              namespaces:
              - "kube-system"
              - "local-path-storage"
              - "zarf"
          - resources:
              namespaces:
              - velero
              kinds:
              - Pod
              selector:
                matchLabels:
                  name: node-agent
        parameters:
          allow:
          - "/var/lib/rancher/k3s/storage/*"
          - "/var/local-path-provisioner/*"
      restrict-host-path-mount-pv:
        exclude:
          any:
          - resources:
              namespaces:
              - "kube-system"
              - "local-path-storage"
              - "zarf"
        parameters:
          allow:
          - "/var/lib/rancher/k3s/storage/*"
          - "/var/local-path-provisioner/*"
      restrict-image-registries:
        skipOverlayMerge: true
        exclude:
          any:
          - resources:
              namespaces:
              - "kube-system"
              - "local-path-storage"
              - "zarf"
          # Allow the k3s loadbalancer to work
          - resources:
              namespaces:
              - istio-system
              kinds:
              - Pod
              selector:
                matchLabels:
                  svccontroller.k3s.cattle.io/svcname: public-ingressgateway
        parameters:
          allow:
            - "${ZARF_REGISTRY}"
      restrict-volume-types:
        exclude:
          any:
          - resources:
              namespaces:
              - "kube-system"
              - "local-path-storage"
              - "zarf"
      # Allow the k3s loadbalancer to work
      restrict-host-ports:
        exclude:
          any:
          - resources:
              namespaces:
              - istio-system
              kinds:
              - Pod
              selector:
                matchLabels:
                  svccontroller.k3s.cattle.io/svcname: public-ingressgateway
  # Some policies have an autogen rule that also checks upstream controllers
  # (statefulset, daemonset, deployment, etc). Adding this annotation disables
  # these autogen rules so we're only checking pods.
  postRenderers:
    - kustomize:
        patchesJson6902:
        # Required for Zarf
        - target:
            version: v1
            kind: ClusterPolicy
            name: restrict-image-registries
          patch:
          - op: add
            path: "/metadata/annotations/pod-policies.kyverno.io~1autogen-controllers"
            value: none
        # Required for velero when using hostPath backed storage
        - target:
            version: v1
            kind: ClusterPolicy
            name: restrict-host-path-write
          patch:
          - op: add
            path: "/metadata/annotations/pod-policies.kyverno.io~1autogen-controllers"
            value: none
        # Required for the builtin k3s loadbalancer
        - target:
            version: v1
            kind: ClusterPolicy
            name: restrict-host-ports
          patch:
          - op: add
            path: "/metadata/annotations/pod-policies.kyverno.io~1autogen-controllers"
            value: none


kyvernoreporter:
  enabled: true

loki:
  enabled: true
  strategy: "scalable"
  objectStorage:
    endpoint: minio.bbcore-minio.svc.cluster.local
    region: "minio"
    # accessKey: "" -- Added in environment-bb/values.yaml
    # accessSecret: "" -- Added in environment-bb/values.yaml
    bucketNames:
      chunks: loki-logs
      ruler: loki-ruler
      admin: loki-admin
  values:
    read:
      replicas: 1
      resources:
        requests:
          cpu: 300m
          memory: 2Gi
        limits:
          cpu: 10
          memory: 3Gi
    write:
      replicas: 1
      resources:
        requests:
          cpu: 300m
          memory: 2Gi
        limits:
          cpu: 10
          memory: 3Gi
    minio:
      enabled: false
    istio:
      mtls:
        mode: PERMISSIVE

promtail:
  enabled: true
  values:
    istio:
      mtls:
        mode: PERMISSIVE
    resources:
      limits:
        cpu: "500m"
        memory: "750Mi"
      requests:
        cpu: "100m"
        memory: "256Mi"

# TODO!
neuvector:
  enabled: false

monitoring:
  enabled: true
  values:
    istio:
      mtls:
        mode: PERMISSIVE
    cleanUpgrade:
      resources:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "256Mi"
          cpu: "200m"
    alertmanager:
      alertmanagerSpec:
        resources:
          limits:
            cpu: "500m"
            memory: "256Mi"
          requests:
            cpu: "100m"
            memory: "256Mi"
    grafana:
      resources:
        limits:
          cpu: "500m"
          memory: "512Mi"
        requests:
          cpu: "100m"
          memory: "256Mi"
      sidecar:
        resources:
          limits:
            cpu: "500m"
            memory: "100Mi"
          requests:
            cpu: "50m"
            memory: "50Mi"
      downloadDashboards:
        resources:
          limits:
            cpu: "20m"
            memory: "20Mi"
          requests:
            cpu: "20m"
            memory: "20Mi"
    kube-state-metrics:
      resources:
        limits:
          cpu: "500m"
          memory: "128Mi"
        requests:
          cpu: "10m"
          memory: "128Mi"
    prometheus-node-exporter:
      resources:
        limits:
          cpu: "500m"
          memory: "128Mi"
        requests:
          cpu: "100m"
          memory: "128Mi"
    prometheusOperator:
      admissionWebhooks:
        patch:
          resources:
            limits:
              cpu: "100m"
              memory: "128Mi"
            requests:
              cpu: "50m"
              memory: "128Mi"
        cleanupProxy:
          resources:
            limits:
              cpu: "100m"
              memory: "128Mi"
            requests:
              cpu: "50m"
              memory: "128Mi"
      resources:
        limits:
          cpu: "500m"
          memory: "512Mi"
        requests:
          cpu: "100m"
          memory: "512Mi"
      prometheusConfigReloader:
        resources:
          requests:
            cpu: "50m"
            memory: "128Mi"
          limits:
            cpu: "100m"
            memory: "128Mi"
    prometheus:
      prometheusSpec:
        resources:
          limits:
            cpu: "300m"
            memory: "2Gi"
          requests:
            cpu: "100m"
            memory: "2Gi"

twistlock:
#   # Disabling Twistlock for now since Big Bang now needs a license to be specified for Twistlock to initialize successfully. Will re-evaluate turning it back on later, or possibly swap it out for Neuvector once that becomes available.
  enabled: false
#   values:
#     istio:
#       mtls:
#         mode: PERMISSIVE
#     resources:
#       limits:
#         memory: "1Gi"
#         cpu: "500m"
#       requests:
#         memory: "1Gi"
#         cpu: "100m"

addons:
  minioOperator:
    enabled: true
    values:
      operator:
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
            ephemeral-storage: 500Mi
          limits:
            cpu: 200m
            memory: 256Mi
      istio:
        enabled: true
        mtls:
          mode: PERMISSIVE

  minio:
    enabled: false

  gitlab:
    enabled: true
    istio:
      injection: enabled
    objectStorage:
      type: "minio"
      endpoint: http://minio.gitlab-minio.svc.cluster.local:80
      region: "minio"
      # accessKey: "" -- Added in environment-bb/values-bigbang.yaml
      # accessSecret: "" -- Added in environment-bb/values-bigbang.yaml
    values:
      istio:
        mtls:
          mode: PERMISSIVE
      global:
        minio:
          enabled: false
        appConfig:
          lfs:
            bucket: gitlab-lfs
          backups:
            bucket: gitlab-backups
            tmpBucket: gitlab-backups-tmp
        registry:
          bucket: gitlab-registry
        psql:
          host: "acid-gitlab.gitlab.svc.cluster.local"
          port: 5432
          database: "postgres"
          username: "postgres"
          password:
            secret: "postgres.acid-gitlab.credentials.postgresql.acid.zalan.do"
            key: "password"
        # ## https://docs.gitlab.com/charts/installation/deployment#outgoing-email
        # ## Example Outgoing email server settings
        # smtp:
        #   enabled: true
        #   address: smtp.example.com
        #   port: 587
        #   user_name: "user@example.com"
        #   ## https://docs.gitlab.com/charts/installation/secrets#smtp-password
        #   password:
        #     secret: "gitlab-smtp-credentials"
        #     key: "password"
        #   domain: "example.com"
        #   authentication: "login"
        #   starttls_auto: true
        #   openssl_verify_mode: "peer"
        #   pool: false
        # ## https://docs.gitlab.com/charts/installation/deployment#outgoing-email
        # ## Email persona used in email sent by GitLab
        # email:
        #   from: "gitlabadmin@example.com"
        #   display_name: "GitLab Admin"
        #   reply_to: "gitlabadmin@example.com"
        #   subject_suffix: "gitlab"
        redis:
          password:
            enabled: false
          host: mymaster
          serviceName: redis
          port: 26379
          sentinels:
            - host: gitlab-redis-node-0.gitlab-redis-headless.gitlab-redis.svc.cluster.local
              port: 26379
            - host: gitlab-redis-node-1.gitlab-redis-headless.gitlab-redis.svc.cluster.local
              port: 26379
            - host: gitlab-redis-node-2.gitlab-redis-headless.gitlab-redis.svc.cluster.local
              port: 26379
      upgradeCheck:
        resources:
          requests:
            cpu: 500m
            memory: 500Mi
          limits:
            cpu: 500m
            memory: 500Mi
      redis:
        install: false
      postgresql:
        install: false
      registry:
        relativeurls: true
        storage:
          redirect:
            disable: true
        init:
          resources:
            limits:
              cpu: 200m
              memory: 200Mi
            requests:
              cpu: 200m
              memory: 200Mi
        resources:
          limits:
            cpu: 200m
            memory: 1024Mi
          requests:
            cpu: 200m
            memory: 1024Mi
      shared-secrets:
        resources:
          requests:
            cpu: 300m
            memory: 200Mi
          limits:
            cpu: 300m
            memory: 200Mi
      gitlab:
        toolbox:
          init:
            resources:
              requests:
                cpu: 200m
                memory: 200Mi
              limits:
                cpu: 200m
                memory: 200Mi
          resources:
            requests:
              cpu: 2
              memory: 3.5Gi
            limits:
              cpu: 2
              memory: 3.5Gi
          backups:
            cron:
              enabled: true
              # persistence settings for backups
              persistence:
                enabled: true
                accessMode: ReadWriteOnce
                # toolbox.persistence.size should be the same values as this
                # It needs to be sized appropriately for the user's needs. A large deployment with lots of users will
                # need enough space to temporarily store everything in GitLab including LFS, artifacts, and registry.
                # If the size of this volume is too small, the backup will fail. If the size of the restore persistence
                # below is too small, any restore attempt will fail. The space is not used for anything else, and is not
                # used unless a backup/restore is actively happening. That's wasted space that needs to be provisioned in
                # disconnected environments, but it is a benefit if EFS is being used since EFS is dynamically sized.
                # 10Gi is a demo value. Expected production values are 100Gi to 1000Gi or more depending on use case.
                # If you're over that upper value, you should consider using a different backup solution as 1+ TB backup
                # artifacts will be extremely unwieldy.
                size: 10Gi
                # storageClass: your-storage-class-here
              resources:
                requests:
                  cpu: 1
                  memory: 1Gi
                limits:
                  cpu: 10
                  memory: 2Gi
              schedule: "0 1 * * *"
            objectStorage:
              backend: s3
              config:
                secret: "gitlab-minio-s3cfg"
                key: ".s3cfg"
          # persistence settings for restore
          persistence:
            enabled: true
            accessMode: ReadWriteOnce
            size: 10Gi
            # storageClass: your-storage-class-here
        gitlab-exporter:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            limits:
              cpu: 150m
              memory: 200Mi
            requests:
              cpu: 150m
              memory: 200Mi
        migrations:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 1Gi
        webservice:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            limits:
              cpu: 600m
              memory: 2.5Gi
            requests:
              cpu: 600m
              memory: 2.5Gi
          workhorse:
            resources:
              limits:
                cpu: 600m
                memory: 2.5Gi
              requests:
                cpu: 600m
                memory: 2.5Gi
        sidekiq:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            requests:
              memory: 3Gi
              cpu: 1500m
            limits:
              memory: 3Gi
              cpu: 1500m
        gitaly:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            requests:
              cpu: 400m
              memory: 600Mi
            limits:
              cpu: 400m
              memory: 600Mi
          # Uncomment this if you want to specify a particular storage class
#          persistence:
#            storageClass: "your-storage-class-name-here"
        gitlab-shell:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            limits:
              cpu: 300m
              memory: 300Mi
            requests:
              cpu: 300m
              memory: 300Mi
        praefect:
          init:
            resources:
              limits:
                cpu: 200m
                memory: 200Mi
              requests:
                cpu: 200m
                memory: 200Mi
          resources:
            requests:
              cpu: 1
              memory: 1Gi
            limits:
              cpu: 1
              memory: 1Gi
  gitlabRunner:
    enabled: true
    values:
      istio:
        mtls:
          mode: PERMISSIVE
      resources:
        limits:
          memory: 256Mi
          cpu: 200m
        requests:
          memory: 256Mi
          cpu: 200m

  velero:
    enabled: true
    plugins:
      - aws
    values:
      initContainers:
       - name: velero-plugin-for-aws
         image: registry1.dso.mil/ironbank/opensource/velero/velero-plugin-for-aws:v1.6.0
         imagePullPolicy: IfNotPresent
         volumeMounts:
           - mountPath: /target
             name: plugins
         resources:
           requests:
             memory: 512Mi
             cpu: 100m
           limits:
             memory: 512Mi
             cpu: 100m
      resources:
        # Demo/PoC values. Adjust for production
        requests:
          cpu: 100m
          memory: 256Mi
        limits:
          cpu: 10
          memory: 512Mi
      configuration:
        provider: aws
        backupStorageLocation:
          bucket: velero-backups
          config:
            region: minio
            s3ForcePathStyle: "true"
            s3Url: http://minio.velero-minio.svc.cluster.local:80
        volumeSnapshotLocation:
          name: default
          config:
            region: minio
        defaultVolumesToFsBackup: true
        default-repo-maintain-frequency: 168h
      deployNodeAgent: true
      nodeAgent:
        # Demo/PoC values. Adjust for production
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 10
            memory: 1024Mi
      #schedules:

  # Allow the metrics server (if deployed) to accept insecure connections to the
  # kubelet. This allows the metrics server to work with KinD.
  metricsServer:
    values:
      args:
        - --kubelet-insecure-tls
